# @package _global_
MODEL:
  num_layers: 12
  num_in_channels: 1
  num_mid_channels: 16
  num_out_channels: 32
  num_channels_R: 1 # or 1
  num_nlayers: 1
  num_degrees: 2
  edge_dim: 0
  encoder_only: False
  n_heads: 1  # in the "pre_modules"
  vector_attention: False
  down_conv:
      module_type: SE3TBlock
      edge_dim: 0
      div: 4
      n_heads: 1
      knn: True
      num_degrees: 2
      npoint: [256, 128, 64, 32]
      radius: [[0.1], [0.2], [0.4], [0.8]]
      nsamples: [[10], [16], [16], [16]]  #TOD16
      vector_attention: False
      down_conv_nn:
          [
              [
                  [16, 16, 16], # 512 -> 256
              ],
              [
                  [16, 32, 32], # 256 -> 128
              ],
              [
                  [32, 64, 64], #
              ],
              [
                  [64, 64, 64],
              ],
          ]
  up_conv:
      module_type: GraphFPModule # may change into
      edge_dim: 0
      div: 4
      n_heads: 1
      knn: True
      num_degrees: 2
      vector_attention: False
      up_conv_nn:
          [
              [64 + 64, 64],   #
              [64 + 32, 32],
              [32 + 16, 32],   # 128 -> 512
              [32 + 16, 32],   # 512 -> 1024
          ]
  mlp_cls:
      nn: [128]
      dropout: 0.5
