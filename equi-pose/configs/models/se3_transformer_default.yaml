# @package _global_
# Network arguments
model:
  input_num: 512
  dropout_rate: 0.
  init_method: "xavier"
  kpconv: False
  kanchor: 60
  normals: False # action='store_true', help='If set, add normals to the input')
  flag: 'rotation'    # help='pooling method: max | mean | attention | rotation')
  representation: 'quat' #ortho6d
  pooling_method: 'mean'

MODEL:
  num_layers: 12
  num_in_channels: 1
  num_mid_channels: 16
  num_out_channels: 32
  num_channels_R: 2 # or 1
  num_channels_T: 1
  num_nlayers: 1
  num_degrees: 2
  edge_dim: 0
  encoder_only: False
  n_heads: 4  # in the "pre_modules"
  vector_attention: False
  down_conv:
      module_type: SE3TBlock
      edge_dim: 0
      div: 4
      n_heads: 4
      knn: True
      num_degrees: 2
      npoint: [256, 64, 32, 16]
      radius: [[0.1], [0.2], [0.4], [0.8]]
      nsamples: [[10], [16], [16], [15]]  #TOD16
      vector_attention: False
      down_conv_nn:
          [
              [
                  [16, 16, 16], # 512 -> 256
              ],
              [
                  [16, 32, 32], # 256 -> 128
              ],
              [
                  [32, 64, 64], #
              ],
              [
                  [64, 64, 64],
              ],
          ]
  up_conv:
      module_type: GraphFPModule # may change into
      edge_dim: 0
      div: 4
      n_heads: 4
      knn: True
      num_degrees: 2
      vector_attention: False
      up_conv_nn:
          [
              [64 + 64, 64],   #
              [64 + 32, 32],
              [32 + 16, 32],   # 128 -> 512
              [32 + 16, 32],   # 512 -> 1024
          ]
  mlp_cls:
      nn: [128]
      dropout: 0.5
